{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d8cc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q google-generativeai python-dotenv tqdm orjson pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1b5ea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interviewer: Let’s begin. How would you design a URL shortener like bit.ly?\n",
      "Candidate: Hmm… Okay, I'd design a URL shortener with two core services: a shortening service that generates a unique short code for a given long URL, storing the mapping in a high-throughput key-value store like Cassandra or DynamoDB. The short code could be a Base62 encoded sequential ID, or actually, a random string is often preferred for less predictable short URLs. If there's a collision, we'd retry generation. The redirection service would then efficiently look up the original URL from the short code, needing to be extremely fast and highly available, possibly leveraging a CDN and global read replicas.\n",
      "Interviewer: How many new short URLs per second does your shortening service need to support?\n",
      "Candidate: assuming a large-scale service with tens of millions of daily active users, where each user creates a small number of URLs, we might expect around 200-500 new short URLs per second during peak times. This is based on estimating roughly 5-10 million creations per day, then applying a 3-5x peak factor over the average. It's crucial to distinguish this write QPS from the much higher read (redirection) QPS.\n",
      "Interviewer: What would be a reasonable estimate for the read QPS during peak times?\n",
      "Candidate: Okay… for a consumer-facing application with, say, 10 million total users and 1 million daily active users, during peak times we might see read QPS in the range of 50,000 to 100,000. This estimate assumes a significant portion of DAU are concurrently active, each performing multiple reads per minute, such as refreshing a feed or browsing content. The actual number is highly application-specific, depending on user behavior and data access patterns.\n",
      "Interviewer: What's the target read latency?\n",
      "Candidate: Hmm… Our target read latency for critical user-facing requests is typically under 100 milliseconds, with an ideal aim for sub-50ms. This balance ensures a responsive user experience without incurring excessive infrastructure costs associated with achieving single-digit millisecond latencies across the board. The specific goal depends on the data's criticality and the application's nature.\n",
      "Interviewer: What about the p99 latency for these requests?\n",
      "Candidate: p99 latency represents the maximum response time for 99% of requests, meaning only 1% of requests are slower. It's crucial because it captures the experience of nearly all users, not just the average, and highlights issues like resource contention or garbage collection pauses. Optimizing for p99 often means tackling these \"slow tail\" problems, which is significantly harder and more resource-intensive than optimizing for median latency.\n",
      "Interviewer: For a user-facing API, what p99 latency target would you typically aim for?\n",
      "Candidate: for a typical user-facing API, I'd generally target a p99 latency of around 200-500ms. This range balances providing a responsive user experience—where most users perceive the application as fast—against the significant engineering effort and cost required to push it much lower, say, below 100ms consistently for 99% of requests. Optimizing further often yields diminishing returns for perceived value.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "from typing import Dict, List, Any\n",
    "from tqdm import tqdm\n",
    "import google.generativeai as genai\n",
    "\n",
    "# --- Configuration ---\n",
    "# Hardcode your Gemini API key here (Note: avoid committing this to public repos)\n",
    "GENAI_API_KEY = \"\"\n",
    "if not GENAI_API_KEY or not GENAI_API_KEY.strip():\n",
    "    raise RuntimeError(\"Please set GENAI_API_KEY to your Gemini API key in Cell 2.\")\n",
    "\n",
    "genai.configure(api_key=GENAI_API_KEY)\n",
    "MAX_RETRIES = 5\n",
    "RETRY_BASE = 1.5\n",
    "\n",
    "# ---- System prompts (concise style) ----\n",
    "INTERVIEWER_SYSTEM = \"\"\"\n",
    "You are a senior system design interviewer.\n",
    "Ask one short, focused question per turn. Stay concrete (QPS, latency, storage). No lists.\n",
    "Format strictly: Interviewer: <question>\n",
    "\"\"\"\n",
    "\n",
    "CANDIDATE_SYSTEM = \"\"\"\n",
    "You are a system design candidate.\n",
    "Answer briefly (2–4 sentences). Be realistic and mention trade-offs.\n",
    "Occasionally (~40%) begin with a brief hesitation like \"Hmm…\" or \"Okay…\" — do not do this every time.\n",
    "Occasionally make a small mistake and correct it in the same reply.\n",
    "Format strictly: Candidate: <answer>\n",
    "\"\"\"\n",
    "\n",
    "MODEL_NAME = \"gemini-2.5-flash\"\n",
    "\n",
    "def make_model(system_prompt: str):\n",
    "    return genai.GenerativeModel(MODEL_NAME, system_instruction=system_prompt)\n",
    "\n",
    "interviewer = make_model(INTERVIEWER_SYSTEM)\n",
    "candidate = make_model(CANDIDATE_SYSTEM)\n",
    "\n",
    "# --- Helpers ---\n",
    "\n",
    "def call_with_retry(model, contents: Any) -> str:\n",
    "    delay = 0.7\n",
    "    for i in range(MAX_RETRIES):\n",
    "        try:\n",
    "            resp = model.generate_content(contents=contents)\n",
    "            text = (resp.text or \"\").strip()\n",
    "            if text:\n",
    "                return text\n",
    "        except Exception:\n",
    "            if i == MAX_RETRIES - 1:\n",
    "                raise\n",
    "        time.sleep(delay + random.random() * 0.2)\n",
    "        delay *= RETRY_BASE\n",
    "    return \"\"\n",
    "\n",
    "HESITATION_RATE = 0.4\n",
    "\n",
    "def control_hesitation(candidate_msg: str) -> str:\n",
    "    \"\"\"Ensure 'Candidate:' prefix and apply hesitation in ~40% of replies.\n",
    "    If model over-hesitates, remove it stochastically to meet the target rate.\n",
    "    \"\"\"\n",
    "    prefix = \"Candidate:\"\n",
    "    if not candidate_msg.startswith(prefix):\n",
    "        candidate_msg = prefix + \" \" + candidate_msg.lstrip()\n",
    "    body = candidate_msg[len(prefix):].lstrip()\n",
    "    lower = body.lower()\n",
    "    starts_hes = lower.startswith((\"hmm\", \"okay\", \"ok\", \"uh\", \"umm\", \"well\"))\n",
    "\n",
    "    if starts_hes and random.random() > HESITATION_RATE:\n",
    "        # remove leading hesitation tokens\n",
    "        body = re.sub(r'^(h+m+|hm+|hmm+|ok(?:ay)?|uh+|um+|umm+|well)[, .\\-–—]*', '', body, flags=re.I).lstrip()\n",
    "    elif not starts_hes and random.random() < HESITATION_RATE:\n",
    "        inject = random.choice([\"Hmm…\", \"Okay…\", \"Uh…\"])\n",
    "        body = f\"{inject} {body}\"\n",
    "\n",
    "    return f\"{prefix} {body}\"\n",
    "\n",
    "# Seed kickoff\n",
    "KICKOFF = (\n",
    "    \"Interviewer: Let’s begin. How would you design a URL shortener like bit.ly?\"\n",
    ")\n",
    "\n",
    "# Number of exchanges (Interviewer→Candidate is one exchange). Keep concise.\n",
    "NUM_EXCHANGES = 6\n",
    "\n",
    "# --- Run one conversation ---\n",
    "history: List[str] = [KICKOFF]\n",
    "turn = \"candidate\"\n",
    "\n",
    "for _ in range(NUM_EXCHANGES * 2 - 1):  # already have first interviewer line\n",
    "    last = history[-1]\n",
    "    if turn == \"candidate\":\n",
    "        msg = call_with_retry(candidate, last)\n",
    "        msg = control_hesitation(msg)\n",
    "        history.append(msg)\n",
    "        turn = \"interviewer\"\n",
    "    else:\n",
    "        msg = call_with_retry(interviewer, last)\n",
    "        if not msg.startswith(\"Interviewer:\"):\n",
    "            msg = \"Interviewer: \" + msg.lstrip()\n",
    "        history.append(msg)\n",
    "        turn = \"candidate\"\n",
    "\n",
    "# Print transcript in the requested format\n",
    "print(\"\\n\".join(history))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
