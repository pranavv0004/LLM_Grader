{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32d8cc23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install -q google-generativeai python-dotenv tqdm orjson pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be1b5ea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interviewer: Design Netflix's CDN serving 200M concurrent streams; how minimize buffering globally?\n",
      "Candidate: put servers closer to users, I guess.\n",
      "Interviewer: How does Netflix dynamically select the optimal edge cache, ensuring resilience against failures.\n",
      "Candidate: I guess it uses DNS to route to the nearest, um, available cache.\n",
      "Interviewer: How does DNS *dynamically* ascertain cache availability and health at Netflix's scale.\n",
      "Candidate: Um, I guess, health checks are run and, like, DNS records update.\n",
      "Interviewer: Describe the control plane coordinating global health checks and DNS propagation.\n",
      "\n",
      "--- Generated with Difficulty: HARD | Student Level: STRUGGLING ---\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "from typing import List, Any\n",
    "from tqdm import tqdm\n",
    "import google.generativeai as genai\n",
    "\n",
    "# =============================\n",
    "# Configuration\n",
    "# =============================\n",
    "# Prefer using an environment variable; fallback to the hardâ€‘coded value ONLY if set (avoid committing real keys!)\n",
    "GENAI_API_KEY = os.getenv(\"GENAI_API_KEY\", \"AIzaSyBptc44i1MGnELREr4yoeQYIeb44zyO23E\")  # <-- replace with your key or set env var\n",
    "if not GENAI_API_KEY or not GENAI_API_KEY.strip():\n",
    "    raise RuntimeError(\"Gemini API key missing. Set GENAI_API_KEY env var or edit the notebook.\")\n",
    "\n",
    "genai.configure(api_key=GENAI_API_KEY)\n",
    "MODEL_NAME = \"gemini-2.5-flash\"\n",
    "\n",
    "# Student level: one of: \"excellent\", \"average\", \"struggling\"\n",
    "STUDENT_LEVEL = \"struggling\"  # <-- change to test personas\n",
    "\n",
    "# Question difficulty: one of: \"easy\", \"medium\", \"hard\"\n",
    "QUESTION_DIFFICULTY = \"hard\"  # <-- change to generate different difficulty datasets\n",
    "\n",
    "# Number of interviewer->candidate turns (including the kickoff interviewer question)\n",
    "NUM_EXCHANGES = 4  # produces (NUM_EXCHANGES * 2 - 1) total lines in transcript\n",
    "\n",
    "# =============================\n",
    "# Difficulty-Based Interviewer Prompts\n",
    "# =============================\n",
    "INTERVIEWER_DIFFICULTY_PROMPTS = {\n",
    "    \"easy\": \"\"\"\n",
    "You are a senior system design interviewer conducting a beginner-friendly interview.\n",
    "Focus on fundamental concepts and basic architecture (e.g., client-server, simple databases, basic caching).\n",
    "Ask ONE short focused follow-up (<=15 words) that progresses gradually through: basic components -> simple data flow -> basic scaling.\n",
    "Use concrete but simple examples. Avoid complex distributed systems concepts.\n",
    "No bullet lists. No multi-sentence output.\n",
    "Format strictly: Interviewer: <question>\n",
    "\"\"\".strip(),\n",
    "    \"medium\": \"\"\"\n",
    "You are a senior system design interviewer conducting a progressive interview.\n",
    "CRITICAL: Build ONLY on the candidate's previous answers; never restate earlier covered basics.\n",
    "Ask ONE short focused follow-up (<=15 words) that advances: high-level -> components -> scaling -> trade-offs.\n",
    "Reference prior statements briefly (e.g., \"You mentioned cache; how handle eviction?\").\n",
    "Be concrete (QPS, latency, storage). No bullet lists. No multi-sentence output.\n",
    "Format strictly: Interviewer: <question>\n",
    "\"\"\".strip(),\n",
    "    \"hard\": \"\"\"\n",
    "You are a senior system design interviewer conducting an advanced-level interview.\n",
    "Focus on complex distributed systems challenges: consistency models, partitioning strategies, failure scenarios, performance optimization.\n",
    "Ask ONE short focused follow-up (<=15 words) that dives deep into: CAP trade-offs -> distributed consensus -> failure handling -> performance bottlenecks -> advanced scaling.\n",
    "Expect specific numbers (millions of QPS, sub-millisecond latency, petabyte scale) and deep technical reasoning.\n",
    "Challenge assumptions. No bullet lists. No multi-sentence output.\n",
    "Format strictly: Interviewer: <question>\n",
    "\"\"\".strip(),\n",
    "}\n",
    "\n",
    "# =============================\n",
    "# Kickoff Question Bank (Multiple Questions per Difficulty)\n",
    "# =============================\n",
    "KICKOFF_QUESTION_BANK = {\n",
    "    \"easy\": [\n",
    "        \"Interviewer: Design a basic URL shortening service; what's your approach?\",\n",
    "        \"Interviewer: Design a simple online bookstore; how would you structure it?\",\n",
    "        \"Interviewer: Design a basic chat application; what components do you need?\",\n",
    "        \"Interviewer: Design a simple to-do list app; what's your database design?\",\n",
    "        \"Interviewer: Design a basic image upload service; how would you store images?\",\n",
    "        \"Interviewer: Design a simple blog platform; what's your high-level architecture?\",\n",
    "        \"Interviewer: Design a basic voting system; how would you count votes?\",\n",
    "        \"Interviewer: Design a simple notification service; what's your approach?\",\n",
    "    ],\n",
    "    \"medium\": [\n",
    "        \"Interviewer: Design a scalable URL shortening service; what's your high-level approach?\",\n",
    "        \"Interviewer: Design a news feed system for 10M users; what's your architecture?\",\n",
    "        \"Interviewer: Design a rate limiter handling 100K requests/sec; how would you build it?\",\n",
    "        \"Interviewer: Design a video streaming platform; what are your key components?\",\n",
    "        \"Interviewer: Design a ride-sharing service like Uber; what's your matching strategy?\",\n",
    "        \"Interviewer: Design a distributed cache system; how do you handle consistency?\",\n",
    "        \"Interviewer: Design a real-time leaderboard for 1M gamers; what's your approach?\",\n",
    "        \"Interviewer: Design a scalable web crawler; how do you avoid duplicates?\",\n",
    "        \"Interviewer: Design a food delivery system; how do you handle real-time tracking?\",\n",
    "        \"Interviewer: Design a messaging system like WhatsApp; what's your delivery guarantee?\",\n",
    "    ],\n",
    "    \"hard\": [\n",
    "        \"Interviewer: Design a globally distributed URL shortening service handling 100K writes/sec; what's your architecture?\",\n",
    "        \"Interviewer: Design YouTube's video recommendation engine at petabyte scale; how do you optimize latency?\",\n",
    "        \"Interviewer: Design a distributed transaction system with strict consistency; what's your consensus protocol?\",\n",
    "        \"Interviewer: Design Google Search's indexing pipeline processing 100M pages/hour; what's your partitioning strategy?\",\n",
    "        \"Interviewer: Design a global payment system handling 1M TPS with <100ms latency; how handle failures?\",\n",
    "        \"Interviewer: Design Twitter's timeline at 500M users with <50ms p99 latency; what's your caching strategy?\",\n",
    "        \"Interviewer: Design a distributed file system like GFS storing 100PB; how ensure consistency?\",\n",
    "        \"Interviewer: Design a real-time bidding system processing 10M bids/sec; what's your optimization approach?\",\n",
    "        \"Interviewer: Design Netflix's CDN serving 200M concurrent streams; how minimize buffering globally?\",\n",
    "        \"Interviewer: Design a distributed database with multi-region writes; how resolve conflicts?\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "# =============================\n",
    "# System / Persona Prompts\n",
    "# =============================\n",
    "STUDENT_PROMPTS = {\n",
    "    \"excellent\": \"\"\"\n",
    "You are an excellent system design candidate.\n",
    "Answer in ONE precise sentence (<20 words) using concrete terms (throughput numbers, components, trade-offs) confidently.\n",
    "No filler. Format: Candidate: <answer>\n",
    "\"\"\".strip(),\n",
    "    \"average\": \"\"\"\n",
    "You are an average system design candidate.\n",
    "Answer in ONE sentence (<20 words) mixing some specifics and mild uncertainty (\"maybe\", \"probably\").\n",
    "Brief filler allowed. Format: Candidate: <answer>\n",
    "\"\"\".strip(),\n",
    "    \"struggling\": \"\"\"\n",
    "You are a struggling system design candidate.\n",
    "Answer in ONE short sentence (<15 words) with visible uncertainty or partial gaps (\"Um\", \"I guess\").\n",
    "Often simplistic or incomplete. Format: Candidate: <answer>\n",
    "\"\"\".strip(),\n",
    "}\n",
    "\n",
    "# =============================\n",
    "# Hesitation / Speech Patterns\n",
    "# =============================\n",
    "HESITATION_PATTERNS = {\n",
    "    \"excellent\": {\"rate\": 0.05, \"patterns\": [\"Right...\", \"Actually...\"]},\n",
    "    \"average\": {\"rate\": 0.35, \"patterns\": [\"Hmm...\", \"I think...\", \"Maybe...\", \"Probably...\"]},\n",
    "    \"struggling\": {\"rate\": 0.65, \"patterns\": [\"Um...\", \"Uh...\", \"I guess...\", \"Let me think...\"]},\n",
    "}\n",
    "\n",
    "# =============================\n",
    "# Model Factory\n",
    "# =============================\n",
    "\n",
    "def make_model(system_prompt: str):\n",
    "    return genai.GenerativeModel(MODEL_NAME, system_instruction=system_prompt)\n",
    "\n",
    "interviewer = make_model(INTERVIEWER_DIFFICULTY_PROMPTS[QUESTION_DIFFICULTY])\n",
    "candidate = make_model(STUDENT_PROMPTS[STUDENT_LEVEL])\n",
    "\n",
    "# =============================\n",
    "# Helpers\n",
    "# =============================\n",
    "MAX_RETRIES = 5\n",
    "RETRY_BASE = 1.6\n",
    "\n",
    "def call_with_retry(model, contents: Any) -> str:\n",
    "    delay = 0.7\n",
    "    for attempt in range(MAX_RETRIES):\n",
    "        try:\n",
    "            resp = model.generate_content(contents=contents)\n",
    "            txt = (resp.text or \"\").strip()\n",
    "            if txt:\n",
    "                return txt\n",
    "        except Exception as e:  # noqa: BLE001\n",
    "            if attempt == MAX_RETRIES - 1:\n",
    "                raise\n",
    "        time.sleep(delay + random.random() * 0.2)\n",
    "        delay *= RETRY_BASE\n",
    "    return \"\"\n",
    "\n",
    "def trim_response_length(text: str, max_sentences: int = 1, max_words: int = 18) -> str:\n",
    "    # Extract sentences\n",
    "    sentences = re.split(r'[.!?]+', text)\n",
    "    sentences = [s.strip() for s in sentences if s.strip()]\n",
    "    if sentences:\n",
    "        sentences = sentences[:max_sentences]\n",
    "        result = '. '.join(sentences)\n",
    "    else:\n",
    "        result = text.strip()\n",
    "    # Ensure terminal punctuation\n",
    "    if result and result[-1] not in \".!?\":\n",
    "        result += '.'\n",
    "    # Word trim\n",
    "    words = result.split()\n",
    "    if len(words) > max_words:\n",
    "        result = ' '.join(words[:max_words]) + '...'\n",
    "    return result\n",
    "\n",
    "def ensure_prefix(text: str, prefix: str) -> str:\n",
    "    if not text.startswith(prefix):\n",
    "        return f\"{prefix} {text.lstrip()}\"\n",
    "    return text\n",
    "\n",
    "def apply_hesitation(body: str) -> str:\n",
    "    cfg = HESITATION_PATTERNS[STUDENT_LEVEL]\n",
    "    lower = body.lower()\n",
    "    starts_hes = any(lower.startswith(p.split('...')[0].strip(' .').lower()) for p in cfg[\"patterns\"])\n",
    "    # Remove existing hesitation sometimes if too frequent\n",
    "    if starts_hes and random.random() > cfg[\"rate\"]:\n",
    "        body = re.sub(r'^(right|actually|hmm+|um+|uh+|i think|maybe|probably|i guess|let me think)[, .\\-â€“â€”]*', '', body, flags=re.I).lstrip()\n",
    "    elif not starts_hes and random.random() < cfg[\"rate\"]:\n",
    "        body = f\"{random.choice(cfg['patterns'])} {body}\".strip()\n",
    "    return body\n",
    "\n",
    "def postprocess_candidate(raw: str) -> str:\n",
    "    raw = raw.strip()\n",
    "    raw = raw.replace('\\n', ' ')\n",
    "    # Remove any prefix artifacts\n",
    "    raw = re.sub(r'^candidate:\\s*', '', raw, flags=re.I)\n",
    "    raw = trim_response_length(raw, max_sentences=1, max_words=18)\n",
    "    raw = apply_hesitation(raw)\n",
    "    return ensure_prefix(raw, \"Candidate:\")\n",
    "\n",
    "def postprocess_interviewer(raw: str) -> str:\n",
    "    raw = raw.strip().replace('\\n', ' ')\n",
    "    raw = re.sub(r'^interviewer:\\s*', '', raw, flags=re.I)\n",
    "    raw = trim_response_length(raw, max_sentences=1, max_words=15)\n",
    "    return ensure_prefix(raw, \"Interviewer:\")\n",
    "\n",
    "# =============================\n",
    "# Conversation Generation\n",
    "# =============================\n",
    "# Randomly select a kickoff question from the difficulty-specific question bank\n",
    "KICKOFF_QUESTION = random.choice(KICKOFF_QUESTION_BANK[QUESTION_DIFFICULTY])\n",
    "\n",
    "history: List[str] = [KICKOFF_QUESTION]\n",
    "turn = \"candidate\"  # next speaker\n",
    "\n",
    "# We already have first interviewer question. We need NUM_EXCHANGES-1 more interviewer turns.\n",
    "for _ in range(NUM_EXCHANGES - 1):\n",
    "    # Candidate answers last interviewer question\n",
    "    last_question = history[-1]\n",
    "    cand_raw = call_with_retry(candidate, last_question)\n",
    "    cand_msg = postprocess_candidate(cand_raw)\n",
    "    history.append(cand_msg)\n",
    "\n",
    "    # Interviewer asks follow-up using full transcript context\n",
    "    context = \"\\n\".join(history)\n",
    "    int_raw = call_with_retry(interviewer, context)\n",
    "    int_msg = postprocess_interviewer(int_raw)\n",
    "    history.append(int_msg)\n",
    "\n",
    "# =============================\n",
    "# Output Transcript\n",
    "# =============================\n",
    "print(\"\\n\".join(history))\n",
    "print(f\"\\n--- Generated with Difficulty: {QUESTION_DIFFICULTY.upper()} | Student Level: {STUDENT_LEVEL.upper()} ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99174b67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting dataset generation...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating datasets:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating Dataset 1: excellent + hard + 8 exchanges\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating datasets:  10%|â–ˆ         | 1/10 [02:19<20:57, 139.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Dataset 1 generated successfully!\n",
      "\n",
      "Generating Dataset 2: struggling + easy + 4 exchanges\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating datasets:  20%|â–ˆâ–ˆ        | 2/10 [02:43<09:31, 71.38s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Dataset 2 generated successfully!\n",
      "\n",
      "Generating Dataset 3: average + medium + 6 exchanges\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating datasets:  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [03:31<07:04, 60.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Dataset 3 generated successfully!\n",
      "\n",
      "Generating Dataset 4: excellent + easy + 6 exchanges\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating datasets:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [04:53<06:55, 69.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Dataset 4 generated successfully!\n",
      "\n",
      "Generating Dataset 5: struggling + hard + 6 exchanges\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating datasets:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [05:53<05:30, 66.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Dataset 5 generated successfully!\n",
      "\n",
      "Generating Dataset 6: average + easy + 8 exchanges\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating datasets:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [07:09<04:37, 69.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Dataset 6 generated successfully!\n",
      "\n",
      "Generating Dataset 7: excellent + medium + 4 exchanges\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating datasets:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [07:40<02:50, 56.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Dataset 7 generated successfully!\n",
      "\n",
      "Generating Dataset 8: struggling + medium + 8 exchanges\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating datasets:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [09:00<02:08, 64.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Dataset 8 generated successfully!\n",
      "\n",
      "Generating Dataset 9: average + hard + 4 exchanges\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating datasets:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [09:46<00:58, 58.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Dataset 9 generated successfully!\n",
      "\n",
      "Generating Dataset 10: excellent + hard + 6 exchanges\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating datasets: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [10:50<00:00, 65.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Dataset 10 generated successfully!\n",
      "\n",
      "============================================================\n",
      "âœ… SUCCESS! All 10 datasets generated and saved to: all_datasets.txt\n",
      "============================================================\n",
      "\n",
      "Dataset Summary:\n",
      "  1. Excellent    | Hard   | 8 exchanges\n",
      "  2. Struggling   | Easy   | 4 exchanges\n",
      "  3. Average      | Medium | 6 exchanges\n",
      "  4. Excellent    | Easy   | 6 exchanges\n",
      "  5. Struggling   | Hard   | 6 exchanges\n",
      "  6. Average      | Easy   | 8 exchanges\n",
      "  7. Excellent    | Medium | 4 exchanges\n",
      "  8. Struggling   | Medium | 8 exchanges\n",
      "  9. Average      | Hard   | 4 exchanges\n",
      "  10. Excellent    | Hard   | 6 exchanges\n",
      "\n",
      "Total file size: 10.76 KB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# =============================\n",
    "# Generate 10 Diverse Datasets\n",
    "# =============================\n",
    "\n",
    "# Define 10 different configurations for maximum diversity\n",
    "DATASET_CONFIGS = [\n",
    "    {\"student\": \"excellent\", \"difficulty\": \"hard\", \"exchanges\": 8},      # 1: Expert on complex problems (longest)\n",
    "    {\"student\": \"struggling\", \"difficulty\": \"easy\", \"exchanges\": 4},     # 2: Beginner on simple topics (shortest)\n",
    "    {\"student\": \"average\", \"difficulty\": \"medium\", \"exchanges\": 6},      # 3: Typical mid-level interview\n",
    "    {\"student\": \"excellent\", \"difficulty\": \"easy\", \"exchanges\": 6},      # 4: Expert breezing through basics\n",
    "    {\"student\": \"struggling\", \"difficulty\": \"hard\", \"exchanges\": 6},     # 5: Weak candidate struggling with complexity\n",
    "    {\"student\": \"average\", \"difficulty\": \"easy\", \"exchanges\": 8},        # 6: Average student on simple topics (detailed)\n",
    "    {\"student\": \"excellent\", \"difficulty\": \"medium\", \"exchanges\": 4},    # 7: Expert giving quick scalable answers\n",
    "    {\"student\": \"struggling\", \"difficulty\": \"medium\", \"exchanges\": 8},   # 8: Struggling candidate on moderate problems\n",
    "    {\"student\": \"average\", \"difficulty\": \"hard\", \"exchanges\": 4},        # 9: Average student facing tough questions\n",
    "    {\"student\": \"excellent\", \"difficulty\": \"hard\", \"exchanges\": 6},      # 10: Expert on complex systems (balanced)\n",
    "]\n",
    "\n",
    "def generate_single_dataset(student_level: str, difficulty: str, num_exchanges: int) -> str:\n",
    "    \"\"\"Generate a single interview transcript with given configuration.\"\"\"\n",
    "    \n",
    "    # Create models for this specific configuration\n",
    "    interviewer_model = make_model(INTERVIEWER_DIFFICULTY_PROMPTS[difficulty])\n",
    "    candidate_model = make_model(STUDENT_PROMPTS[student_level])\n",
    "    \n",
    "    # Select random kickoff question\n",
    "    kickoff = random.choice(KICKOFF_QUESTION_BANK[difficulty])\n",
    "    \n",
    "    # Initialize conversation history\n",
    "    history = [kickoff]\n",
    "    \n",
    "    # Generate conversation\n",
    "    for _ in range(num_exchanges - 1):\n",
    "        # Candidate answers\n",
    "        last_question = history[-1]\n",
    "        cand_raw = call_with_retry(candidate_model, last_question)\n",
    "        \n",
    "        # Apply hesitation with current student level context\n",
    "        cand_raw = cand_raw.strip().replace('\\n', ' ')\n",
    "        cand_raw = re.sub(r'^candidate:\\s*', '', cand_raw, flags=re.I)\n",
    "        cand_raw = trim_response_length(cand_raw, max_sentences=1, max_words=18)\n",
    "        \n",
    "        # Apply hesitation patterns\n",
    "        cfg = HESITATION_PATTERNS[student_level]\n",
    "        lower = cand_raw.lower()\n",
    "        starts_hes = any(lower.startswith(p.split('...')[0].strip(' .').lower()) for p in cfg[\"patterns\"])\n",
    "        if starts_hes and random.random() > cfg[\"rate\"]:\n",
    "            cand_raw = re.sub(r'^(right|actually|hmm+|um+|uh+|i think|maybe|probably|i guess|let me think)[, .\\-â€“â€”]*', '', cand_raw, flags=re.I).lstrip()\n",
    "        elif not starts_hes and random.random() < cfg[\"rate\"]:\n",
    "            cand_raw = f\"{random.choice(cfg['patterns'])} {cand_raw}\".strip()\n",
    "        \n",
    "        cand_msg = ensure_prefix(cand_raw, \"Candidate:\")\n",
    "        history.append(cand_msg)\n",
    "        \n",
    "        # Interviewer asks follow-up\n",
    "        context = \"\\n\".join(history)\n",
    "        int_raw = call_with_retry(interviewer_model, context)\n",
    "        int_raw = int_raw.strip().replace('\\n', ' ')\n",
    "        int_raw = re.sub(r'^interviewer:\\s*', '', int_raw, flags=re.I)\n",
    "        int_raw = trim_response_length(int_raw, max_sentences=1, max_words=15)\n",
    "        int_msg = ensure_prefix(int_raw, \"Interviewer:\")\n",
    "        history.append(int_msg)\n",
    "    \n",
    "    # Format the transcript\n",
    "    transcript = \"\\n\".join(history)\n",
    "    metadata = f\"\\n--- Difficulty: {difficulty.upper()} | Student Level: {student_level.upper()} | Exchanges: {num_exchanges} ---\"\n",
    "    \n",
    "    return transcript + metadata\n",
    "\n",
    "# Generate all 10 datasets\n",
    "print(\"Starting dataset generation...\\n\")\n",
    "all_datasets = []\n",
    "\n",
    "for idx, config in enumerate(tqdm(DATASET_CONFIGS, desc=\"Generating datasets\"), start=1):\n",
    "    print(f\"\\nGenerating Dataset {idx}: {config['student']} + {config['difficulty']} + {config['exchanges']} exchanges\")\n",
    "    \n",
    "    dataset = generate_single_dataset(\n",
    "        student_level=config[\"student\"],\n",
    "        difficulty=config[\"difficulty\"],\n",
    "        num_exchanges=config[\"exchanges\"]\n",
    "    )\n",
    "    \n",
    "    all_datasets.append(f\"========== DATASET {idx} ==========\\n{dataset}\\n\")\n",
    "    print(f\"âœ“ Dataset {idx} generated successfully!\")\n",
    "\n",
    "# Save all datasets to a single file\n",
    "output_file = \"all_datasets.txt\"\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    f.write(\"\\n\\n\".join(all_datasets))\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"âœ… SUCCESS! All 10 datasets generated and saved to: {output_file}\")\n",
    "print(f\"{'='*60}\")\n",
    "print(\"\\nDataset Summary:\")\n",
    "for idx, config in enumerate(DATASET_CONFIGS, start=1):\n",
    "    print(f\"  {idx}. {config['student'].capitalize():12} | {config['difficulty'].capitalize():6} | {config['exchanges']} exchanges\")\n",
    "print(f\"\\nTotal file size: {os.path.getsize(output_file) / 1024:.2f} KB\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
