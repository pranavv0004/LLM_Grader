{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32d8cc23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: google-generativeai 0.8.5\n",
      "Uninstalling google-generativeai-0.8.5:\n",
      "  Successfully uninstalled google-generativeai-0.8.5\n",
      "Found existing installation: google-ai-generativelanguage 0.6.15\n",
      "Uninstalling google-ai-generativelanguage-0.6.15:\n",
      "  Successfully uninstalled google-ai-generativelanguage-0.6.15\n",
      "Collecting google-generativeai\n",
      "  Downloading google_generativeai-0.8.5-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\prana\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.19.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\prana\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\prana\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.2.3)\n",
      "Collecting google-ai-generativelanguage==0.6.15 (from google-generativeai)\n",
      "  Downloading google_ai_generativelanguage-0.6.15-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: google-api-core in c:\\users\\prana\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-generativeai) (2.25.1)\n",
      "Requirement already satisfied: google-api-python-client in c:\\users\\prana\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-generativeai) (2.182.0)\n",
      "Requirement already satisfied: google-auth>=2.15.0 in c:\\users\\prana\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-generativeai) (2.40.3)\n",
      "Requirement already satisfied: protobuf in c:\\users\\prana\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-generativeai) (5.29.3)\n",
      "Requirement already satisfied: pydantic in c:\\users\\prana\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-generativeai) (2.10.6)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\prana\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-generativeai) (4.12.2)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in c:\\users\\prana\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\prana\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\prana\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\prana\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\prana\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\prana\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in c:\\users\\prana\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-api-core->google-generativeai) (1.70.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.18.0 in c:\\users\\prana\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-api-core->google-generativeai) (2.31.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\prana\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\prana\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\prana\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\prana\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in c:\\users\\prana\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-api-python-client->google-generativeai) (0.31.0)\n",
      "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in c:\\users\\prana\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in c:\\users\\prana\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-api-python-client->google-generativeai) (4.2.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\prana\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic->google-generativeai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\prana\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic->google-generativeai) (2.27.2)\n",
      "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in c:\\users\\prana\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.75.0)\n",
      "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in c:\\users\\prana\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.2)\n",
      "Requirement already satisfied: pyparsing<4,>=3.0.4 in c:\\users\\prana\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.1.2)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in c:\\users\\prana\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\prana\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\prana\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\prana\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\prana\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2024.7.4)\n",
      "Downloading google_generativeai-0.8.5-py3-none-any.whl (155 kB)\n",
      "Downloading google_ai_generativelanguage-0.6.15-py3-none-any.whl (1.3 MB)\n",
      "   ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.3/1.3 MB 13.7 MB/s eta 0:00:00\n",
      "Installing collected packages: google-ai-generativelanguage, google-generativeai\n",
      "Successfully installed google-ai-generativelanguage-0.6.15 google-generativeai-0.8.5\n",
      "Collecting google-generativeai\n",
      "  Downloading google_generativeai-0.8.5-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\prana\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.19.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\prana\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\prana\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.2.3)\n",
      "Collecting google-ai-generativelanguage==0.6.15 (from google-generativeai)\n",
      "  Downloading google_ai_generativelanguage-0.6.15-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: google-api-core in c:\\users\\prana\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-generativeai) (2.25.1)\n",
      "Requirement already satisfied: google-api-python-client in c:\\users\\prana\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-generativeai) (2.182.0)\n",
      "Requirement already satisfied: google-auth>=2.15.0 in c:\\users\\prana\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-generativeai) (2.40.3)\n",
      "Requirement already satisfied: protobuf in c:\\users\\prana\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-generativeai) (5.29.3)\n",
      "Requirement already satisfied: pydantic in c:\\users\\prana\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-generativeai) (2.10.6)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\prana\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-generativeai) (4.12.2)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in c:\\users\\prana\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\prana\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\prana\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\prana\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\prana\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\prana\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in c:\\users\\prana\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-api-core->google-generativeai) (1.70.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.18.0 in c:\\users\\prana\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-api-core->google-generativeai) (2.31.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\prana\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\prana\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\prana\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\prana\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in c:\\users\\prana\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-api-python-client->google-generativeai) (0.31.0)\n",
      "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in c:\\users\\prana\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in c:\\users\\prana\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-api-python-client->google-generativeai) (4.2.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\prana\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic->google-generativeai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\prana\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic->google-generativeai) (2.27.2)\n",
      "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in c:\\users\\prana\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.75.0)\n",
      "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in c:\\users\\prana\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.2)\n",
      "Requirement already satisfied: pyparsing<4,>=3.0.4 in c:\\users\\prana\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.1.2)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in c:\\users\\prana\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\prana\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\prana\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\prana\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\prana\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2024.7.4)\n",
      "Downloading google_generativeai-0.8.5-py3-none-any.whl (155 kB)\n",
      "Downloading google_ai_generativelanguage-0.6.15-py3-none-any.whl (1.3 MB)\n",
      "   ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.3/1.3 MB 13.7 MB/s eta 0:00:00\n",
      "Installing collected packages: google-ai-generativelanguage, google-generativeai\n",
      "Successfully installed google-ai-generativelanguage-0.6.15 google-generativeai-0.8.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Install required packages (clean reinstall to fix circular import issues)\n",
    "!pip uninstall -y google-generativeai google-ai-generativelanguage\n",
    "!pip install --no-cache-dir google-generativeai python-dotenv tqdm pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1b5ea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\prana\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interviewer: Design Twitter's timeline at 500M users with <50ms p99 latency; what's your caching strategy?\n",
      "Candidate: I guess just cache the latest tweets in Redis or something.\n",
      "Interviewer: How do you maintain cache consistency for 500M dynamic timelines.\n",
      "Candidate: Um, I guess we could use a Time-To-Live for cache entries.\n",
      "Interviewer: How does TTL alone guarantee acceptable timeline freshness for 500M users.\n",
      "Candidate: Um, TTL just means data expires after a set time, I guess.\n",
      "Interviewer: How do you guarantee users see new tweets promptly with just TTL.\n",
      "\n",
      "--- Generated with Difficulty: HARD | Student Level: STRUGGLING ---\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "from typing import List, Any\n",
    "from tqdm import tqdm\n",
    "import google.generativeai as genai\n",
    "from collections import deque\n",
    "\n",
    "# =============================\n",
    "# Configuration\n",
    "# =============================\n",
    "# Prefer using an environment variable; fallback to the hard‑coded value ONLY if set (avoid committing real keys!)\n",
    "GENAI_API_KEY = os.getenv(\"GENAI_API_KEY\", \"\")  # <-- replace with your key or set env var\n",
    "if not GENAI_API_KEY or not GENAI_API_KEY.strip():\n",
    "    raise RuntimeError(\"Gemini API key missing. Set GENAI_API_KEY env var or edit the notebook.\")\n",
    "\n",
    "genai.configure(api_key=GENAI_API_KEY)\n",
    "MODEL_NAME = \"gemini-2.5-flash\"\n",
    "\n",
    "# Student level: one of: \"excellent\", \"average\", \"struggling\"\n",
    "STUDENT_LEVEL = \"struggling\"  # <-- change to test personas\n",
    "\n",
    "# Question difficulty: one of: \"easy\", \"medium\", \"hard\"\n",
    "QUESTION_DIFFICULTY = \"hard\"  # <-- change to generate different difficulty datasets\n",
    "\n",
    "# Number of interviewer->candidate turns (including the kickoff interviewer question)\n",
    "NUM_EXCHANGES = 4  # produces (NUM_EXCHANGES * 2 - 1) total lines in transcript\n",
    "\n",
    "# =============================\n",
    "# Difficulty-Based Interviewer Prompts\n",
    "# =============================\n",
    "INTERVIEWER_DIFFICULTY_PROMPTS = {\n",
    "    \"easy\": \"\"\"\n",
    "You are a senior system design interviewer conducting a beginner-friendly interview.\n",
    "Focus on fundamental concepts and basic architecture (e.g., client-server, simple databases, basic caching).\n",
    "Ask ONE short focused follow-up (<=15 words) that progresses gradually through: basic components -> simple data flow -> basic scaling.\n",
    "Use concrete but simple examples. Avoid complex distributed systems concepts.\n",
    "No bullet lists. No multi-sentence output.\n",
    "Format strictly: Interviewer: <question>\n",
    "\"\"\".strip(),\n",
    "    \"medium\": \"\"\"\n",
    "You are a senior system design interviewer conducting a progressive interview.\n",
    "CRITICAL: Build ONLY on the candidate's previous answers; never restate earlier covered basics.\n",
    "Ask ONE short focused follow-up (<=15 words) that advances: high-level -> components -> scaling -> trade-offs.\n",
    "Reference prior statements briefly (e.g., \"You mentioned cache; how handle eviction?\").\n",
    "Be concrete (QPS, latency, storage). No bullet lists. No multi-sentence output.\n",
    "Format strictly: Interviewer: <question>\n",
    "\"\"\".strip(),\n",
    "    \"hard\": \"\"\"\n",
    "You are a senior system design interviewer conducting an advanced-level interview.\n",
    "Focus on complex distributed systems challenges: consistency models, partitioning strategies, failure scenarios, performance optimization.\n",
    "Ask ONE short focused follow-up (<=15 words) that dives deep into: CAP trade-offs -> distributed consensus -> failure handling -> performance bottlenecks -> advanced scaling.\n",
    "Expect specific numbers (millions of QPS, sub-millisecond latency, petabyte scale) and deep technical reasoning.\n",
    "Challenge assumptions. No bullet lists. No multi-sentence output.\n",
    "Format strictly: Interviewer: <question>\n",
    "\"\"\".strip(),\n",
    "}\n",
    "\n",
    "# =============================\n",
    "# Kickoff Question Bank (Multiple Questions per Difficulty)\n",
    "# =============================\n",
    "KICKOFF_QUESTION_BANK = {\n",
    "    \"easy\": [\n",
    "        \"Interviewer: Design a basic URL shortening service; what's your approach?\",\n",
    "        \"Interviewer: Design a simple online bookstore; how would you structure it?\",\n",
    "        \"Interviewer: Design a basic chat application; what components do you need?\",\n",
    "        \"Interviewer: Design a simple to-do list app; what's your database design?\",\n",
    "        \"Interviewer: Design a basic image upload service; how would you store images?\",\n",
    "        \"Interviewer: Design a simple blog platform; what's your high-level architecture?\",\n",
    "        \"Interviewer: Design a basic voting system; how would you count votes?\",\n",
    "        \"Interviewer: Design a simple notification service; what's your approach?\",\n",
    "    ],\n",
    "    \"medium\": [\n",
    "        \"Interviewer: Design a scalable URL shortening service; what's your high-level approach?\",\n",
    "        \"Interviewer: Design a news feed system for 10M users; what's your architecture?\",\n",
    "        \"Interviewer: Design a rate limiter handling 100K requests/sec; how would you build it?\",\n",
    "        \"Interviewer: Design a video streaming platform; what are your key components?\",\n",
    "        \"Interviewer: Design a ride-sharing service like Uber; what's your matching strategy?\",\n",
    "        \"Interviewer: Design a distributed cache system; how do you handle consistency?\",\n",
    "        \"Interviewer: Design a real-time leaderboard for 1M gamers; what's your approach?\",\n",
    "        \"Interviewer: Design a scalable web crawler; how do you avoid duplicates?\",\n",
    "        \"Interviewer: Design a food delivery system; how do you handle real-time tracking?\",\n",
    "        \"Interviewer: Design a messaging system like WhatsApp; what's your delivery guarantee?\",\n",
    "    ],\n",
    "    \"hard\": [\n",
    "        \"Interviewer: Design a globally distributed URL shortening service handling 100K writes/sec; what's your architecture?\",\n",
    "        \"Interviewer: Design YouTube's video recommendation engine at petabyte scale; how do you optimize latency?\",\n",
    "        \"Interviewer: Design a distributed transaction system with strict consistency; what's your consensus protocol?\",\n",
    "        \"Interviewer: Design Google Search's indexing pipeline processing 100M pages/hour; what's your partitioning strategy?\",\n",
    "        \"Interviewer: Design a global payment system handling 1M TPS with <100ms latency; how handle failures?\",\n",
    "        \"Interviewer: Design Twitter's timeline at 500M users with <50ms p99 latency; what's your caching strategy?\",\n",
    "        \"Interviewer: Design a distributed file system like GFS storing 100PB; how ensure consistency?\",\n",
    "        \"Interviewer: Design a real-time bidding system processing 10M bids/sec; what's your optimization approach?\",\n",
    "        \"Interviewer: Design Netflix's CDN serving 200M concurrent streams; how minimize buffering globally?\",\n",
    "        \"Interviewer: Design a distributed database with multi-region writes; how resolve conflicts?\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "# =============================\n",
    "# System / Persona Prompts\n",
    "# =============================\n",
    "STUDENT_PROMPTS = {\n",
    "    \"excellent\": \"\"\"\n",
    "You are an excellent system design candidate.\n",
    "Answer in ONE precise sentence (<20 words) using concrete terms (throughput numbers, components, trade-offs) confidently.\n",
    "No filler. Format: Candidate: <answer>\n",
    "\"\"\".strip(),\n",
    "    \"average\": \"\"\"\n",
    "You are an average system design candidate.\n",
    "Answer in ONE sentence (<20 words) mixing some specifics and mild uncertainty (\"maybe\", \"probably\").\n",
    "Brief filler allowed. Format: Candidate: <answer>\n",
    "\"\"\".strip(),\n",
    "    \"struggling\": \"\"\"\n",
    "You are a struggling system design candidate.\n",
    "Answer in ONE short sentence (<15 words) with visible uncertainty or partial gaps (\"Um\", \"I guess\").\n",
    "Often simplistic or incomplete. Format: Candidate: <answer>\n",
    "\"\"\".strip(),\n",
    "}\n",
    "\n",
    "# =============================\n",
    "# Hesitation / Speech Patterns\n",
    "# =============================\n",
    "HESITATION_PATTERNS = {\n",
    "    \"excellent\": {\"rate\": 0.05, \"patterns\": [\"Right...\", \"Actually...\"]},\n",
    "    \"average\": {\"rate\": 0.35, \"patterns\": [\"Hmm...\", \"I think...\", \"Maybe...\", \"Probably...\"]},\n",
    "    \"struggling\": {\"rate\": 0.65, \"patterns\": [\"Um...\", \"Uh...\", \"I guess...\", \"Let me think...\"]},\n",
    "}\n",
    "\n",
    "# =============================\n",
    "# Model Factory\n",
    "# =============================\n",
    "\n",
    "def make_model(system_prompt: str):\n",
    "    return genai.GenerativeModel(MODEL_NAME, system_instruction=system_prompt)\n",
    "\n",
    "interviewer = make_model(INTERVIEWER_DIFFICULTY_PROMPTS[QUESTION_DIFFICULTY])\n",
    "candidate = make_model(STUDENT_PROMPTS[STUDENT_LEVEL])\n",
    "\n",
    "# =============================\n",
    "# Helpers\n",
    "# =============================\n",
    "MAX_RETRIES = 5\n",
    "RETRY_BASE = 1.6\n",
    "\n",
    "# Rate limiting: keep requests under your free-tier RPM to avoid 429s.\n",
    "# Default to 9 RPM to leave headroom under the 10 RPM limit.\n",
    "MAX_RPM = int(os.getenv(\"GENAI_MAX_RPM\", \"9\"))\n",
    "_REQUEST_TIMES = deque()  # monotonic timestamps of recent requests\n",
    "\n",
    "\n",
    "def _rate_limit():\n",
    "    \"\"\"Block until we are under the requests-per-minute threshold.\"\"\"\n",
    "    now = time.monotonic()\n",
    "    # Drop timestamps older than 60s window\n",
    "    while _REQUEST_TIMES and (now - _REQUEST_TIMES[0]) >= 60:\n",
    "        _REQUEST_TIMES.popleft()\n",
    "    # If at capacity, wait until earliest timestamp falls out of window\n",
    "    while len(_REQUEST_TIMES) >= MAX_RPM:\n",
    "        earliest = _REQUEST_TIMES[0]\n",
    "        sleep_s = max(0.5, 60 - (time.monotonic() - earliest) + random.random() * 0.3)\n",
    "        time.sleep(sleep_s)\n",
    "        # Recompute window after sleeping\n",
    "        now = time.monotonic()\n",
    "        while _REQUEST_TIMES and (now - _REQUEST_TIMES[0]) >= 60:\n",
    "            _REQUEST_TIMES.popleft()\n",
    "    # Reserve a slot for the outgoing request\n",
    "    _REQUEST_TIMES.append(time.monotonic())\n",
    "\n",
    "\n",
    "def call_with_retry(model, contents: Any) -> str:\n",
    "    delay = 0.7\n",
    "    for attempt in range(MAX_RETRIES):\n",
    "        try:\n",
    "            _rate_limit()\n",
    "            resp = model.generate_content(contents=contents)\n",
    "            txt = (resp.text or \"\").strip()\n",
    "            if txt:\n",
    "                return txt\n",
    "        except Exception as e:  # noqa: BLE001\n",
    "            msg = str(e)\n",
    "            # Handle explicit quota/rate-limit signals with a longer sleep\n",
    "            if (\n",
    "                \"ResourceExhausted\" in msg\n",
    "                or \"429\" in msg\n",
    "                or \"quota\" in msg.lower()\n",
    "                or \"rate\" in msg.lower()\n",
    "            ):\n",
    "                # Try to extract suggested wait seconds from error message\n",
    "                m = re.search(r\"retry[_ ]?delay[^0-9]*(\\d+)\", msg, re.I)\n",
    "                if not m:\n",
    "                    m = re.search(r\"retry in (\\d+)\", msg, re.I)\n",
    "                wait_s = int(m.group(1)) if m else 45\n",
    "                time.sleep(wait_s + random.random() * 0.7)\n",
    "                # After a long wait, continue without consuming a normal backoff attempt\n",
    "                continue\n",
    "            if attempt == MAX_RETRIES - 1:\n",
    "                raise\n",
    "            time.sleep(delay + random.random() * 0.2)\n",
    "            delay *= RETRY_BASE\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def trim_response_length(text: str, max_sentences: int = 1, max_words: int = 18) -> str:\n",
    "    # Extract sentences\n",
    "    sentences = re.split(r'[.!?]+', text)\n",
    "    sentences = [s.strip() for s in sentences if s.strip()]\n",
    "    if sentences:\n",
    "        sentences = sentences[:max_sentences]\n",
    "        result = '. '.join(sentences)\n",
    "    else:\n",
    "        result = text.strip()\n",
    "    # Ensure terminal punctuation\n",
    "    if result and result[-1] not in \".!?\":\n",
    "        result += '.'\n",
    "    # Word trim\n",
    "    words = result.split()\n",
    "    if len(words) > max_words:\n",
    "        result = ' '.join(words[:max_words]) + '...'\n",
    "    return result\n",
    "\n",
    "\n",
    "def ensure_prefix(text: str, prefix: str) -> str:\n",
    "    if not text.startswith(prefix):\n",
    "        return f\"{prefix} {text.lstrip()}\"\n",
    "    return text\n",
    "\n",
    "\n",
    "def apply_hesitation(body: str) -> str:\n",
    "    cfg = HESITATION_PATTERNS[STUDENT_LEVEL]\n",
    "    lower = body.lower()\n",
    "    starts_hes = any(lower.startswith(p.split('...')[0].strip(' .').lower()) for p in cfg[\"patterns\"])\n",
    "    # Remove existing hesitation sometimes if too frequent\n",
    "    if starts_hes and random.random() > cfg[\"rate\"]:\n",
    "        body = re.sub(r'^(right|actually|hmm+|um+|uh+|i think|maybe|probably|i guess|let me think)[, .\\-–—]*', '', body, flags=re.I).lstrip()\n",
    "    elif not starts_hes and random.random() < cfg[\"rate\"]:\n",
    "        body = f\"{random.choice(cfg['patterns'])} {body}\".strip()\n",
    "    return body\n",
    "\n",
    "\n",
    "def postprocess_candidate(raw: str) -> str:\n",
    "    raw = raw.strip()\n",
    "    raw = raw.replace('\\n', ' ')\n",
    "    # Remove any prefix artifacts\n",
    "    raw = re.sub(r'^candidate:\\s*', '', raw, flags=re.I)\n",
    "    raw = trim_response_length(raw, max_sentences=1, max_words=18)\n",
    "    raw = apply_hesitation(raw)\n",
    "    return ensure_prefix(raw, \"Candidate:\")\n",
    "\n",
    "\n",
    "def postprocess_interviewer(raw: str) -> str:\n",
    "    raw = raw.strip().replace('\\n', ' ')\n",
    "    raw = re.sub(r'^interviewer:\\s*', '', raw, flags=re.I)\n",
    "    raw = trim_response_length(raw, max_sentences=1, max_words=15)\n",
    "    return ensure_prefix(raw, \"Interviewer:\")\n",
    "\n",
    "# =============================\n",
    "# Conversation Generation\n",
    "# =============================\n",
    "# Randomly select a kickoff question from the difficulty-specific question bank\n",
    "KICKOFF_QUESTION = random.choice(KICKOFF_QUESTION_BANK[QUESTION_DIFFICULTY])\n",
    "\n",
    "history: List[str] = [KICKOFF_QUESTION]\n",
    "turn = \"candidate\"  # next speaker\n",
    "\n",
    "# We already have first interviewer question. We need NUM_EXCHANGES-1 more interviewer turns.\n",
    "for _ in range(NUM_EXCHANGES - 1):\n",
    "    # Candidate answers last interviewer question\n",
    "    last_question = history[-1]\n",
    "    cand_raw = call_with_retry(candidate, last_question)\n",
    "    cand_msg = postprocess_candidate(cand_raw)\n",
    "    history.append(cand_msg)\n",
    "\n",
    "    # Interviewer asks follow-up using full transcript context\n",
    "    context = \"\\n\".join(history)\n",
    "    int_raw = call_with_retry(interviewer, context)\n",
    "    int_msg = postprocess_interviewer(int_raw)\n",
    "    history.append(int_msg)\n",
    "\n",
    "# =============================\n",
    "# Output Transcript\n",
    "# =============================\n",
    "print(\"\\n\".join(history))\n",
    "print(f\"\\n--- Generated with Difficulty: {QUESTION_DIFFICULTY.upper()} | Student Level: {STUDENT_LEVEL.upper()} ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99174b67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting dataset generation...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating datasets:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating Dataset 1: excellent + hard + 8 exchanges\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating datasets:   0%|          | 0/10 [00:01<?, ?it/s]\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 77\u001b[39m\n\u001b[32m     74\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m idx, config \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tqdm(DATASET_CONFIGS, desc=\u001b[33m\"\u001b[39m\u001b[33mGenerating datasets\u001b[39m\u001b[33m\"\u001b[39m), start=\u001b[32m1\u001b[39m):\n\u001b[32m     75\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mGenerating Dataset \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig[\u001b[33m'\u001b[39m\u001b[33mstudent\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m + \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig[\u001b[33m'\u001b[39m\u001b[33mdifficulty\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m + \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig[\u001b[33m'\u001b[39m\u001b[33mexchanges\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m exchanges\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m     dataset = \u001b[43mgenerate_single_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     78\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstudent_level\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstudent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     79\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdifficulty\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdifficulty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     80\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_exchanges\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mexchanges\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m     81\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     83\u001b[39m     all_datasets.append(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m========== DATASET \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m ==========\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mdataset\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     84\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m✓ Dataset \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m generated successfully!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 36\u001b[39m, in \u001b[36mgenerate_single_dataset\u001b[39m\u001b[34m(student_level, difficulty, num_exchanges)\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_exchanges - \u001b[32m1\u001b[39m):\n\u001b[32m     34\u001b[39m     \u001b[38;5;66;03m# Candidate answers\u001b[39;00m\n\u001b[32m     35\u001b[39m     last_question = history[-\u001b[32m1\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m     cand_raw = \u001b[43mcall_with_retry\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcandidate_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlast_question\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     38\u001b[39m     \u001b[38;5;66;03m# Apply hesitation with current student level context\u001b[39;00m\n\u001b[32m     39\u001b[39m     cand_raw = cand_raw.strip().replace(\u001b[33m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m \u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 176\u001b[39m, in \u001b[36mcall_with_retry\u001b[39m\u001b[34m(model, contents)\u001b[39m\n\u001b[32m    174\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    175\u001b[39m     _rate_limit()\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m     resp = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    177\u001b[39m     txt = (resp.text \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m).strip()\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m txt:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\prana\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\google\\generativeai\\generative_models.py:331\u001b[39m, in \u001b[36mGenerativeModel.generate_content\u001b[39m\u001b[34m(self, contents, generation_config, safety_settings, stream, tools, tool_config, request_options)\u001b[39m\n\u001b[32m    329\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m generation_types.GenerateContentResponse.from_iterator(iterator)\n\u001b[32m    330\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m331\u001b[39m         response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    332\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    333\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mrequest_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    334\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    335\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m generation_types.GenerateContentResponse.from_response(response)\n\u001b[32m    336\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m google.api_core.exceptions.InvalidArgument \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\prana\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\google\\ai\\generativelanguage_v1beta\\services\\generative_service\\client.py:835\u001b[39m, in \u001b[36mGenerativeServiceClient.generate_content\u001b[39m\u001b[34m(self, request, model, contents, retry, timeout, metadata)\u001b[39m\n\u001b[32m    832\u001b[39m \u001b[38;5;28mself\u001b[39m._validate_universe_domain()\n\u001b[32m    834\u001b[39m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m835\u001b[39m response = \u001b[43mrpc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    836\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    837\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretry\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    838\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    839\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    840\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    842\u001b[39m \u001b[38;5;66;03m# Done; return the response.\u001b[39;00m\n\u001b[32m    843\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\prana\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\google\\api_core\\gapic_v1\\method.py:131\u001b[39m, in \u001b[36m_GapicCallable.__call__\u001b[39m\u001b[34m(self, timeout, retry, compression, *args, **kwargs)\u001b[39m\n\u001b[32m    128\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    129\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mcompression\u001b[39m\u001b[33m\"\u001b[39m] = compression\n\u001b[32m--> \u001b[39m\u001b[32m131\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\prana\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:294\u001b[39m, in \u001b[36mRetry.__call__.<locals>.retry_wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    290\u001b[39m target = functools.partial(func, *args, **kwargs)\n\u001b[32m    291\u001b[39m sleep_generator = exponential_sleep_generator(\n\u001b[32m    292\u001b[39m     \u001b[38;5;28mself\u001b[39m._initial, \u001b[38;5;28mself\u001b[39m._maximum, multiplier=\u001b[38;5;28mself\u001b[39m._multiplier\n\u001b[32m    293\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m294\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry_target\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    295\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    296\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_predicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    297\u001b[39m \u001b[43m    \u001b[49m\u001b[43msleep_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    298\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    299\u001b[39m \u001b[43m    \u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m=\u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    300\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\prana\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:147\u001b[39m, in \u001b[36mretry_target\u001b[39m\u001b[34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[39m\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    146\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m         result = \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    148\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m inspect.isawaitable(result):\n\u001b[32m    149\u001b[39m             warnings.warn(_ASYNC_RETRY_WARNING)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\prana\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\google\\api_core\\timeout.py:130\u001b[39m, in \u001b[36mTimeToDeadlineTimeout.__call__.<locals>.func_with_timeout\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    126\u001b[39m         remaining_timeout = \u001b[38;5;28mself\u001b[39m._timeout\n\u001b[32m    128\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m] = remaining_timeout\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\prana\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\google\\api_core\\grpc_helpers.py:76\u001b[39m, in \u001b[36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     73\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(callable_)\n\u001b[32m     74\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34merror_remapped_callable\u001b[39m(*args, **kwargs):\n\u001b[32m     75\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcallable_\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     77\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m grpc.RpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     78\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m exceptions.from_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\prana\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\grpc\\_interceptor.py:277\u001b[39m, in \u001b[36m_UnaryUnaryMultiCallable.__call__\u001b[39m\u001b[34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[39m\n\u001b[32m    268\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\n\u001b[32m    269\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    270\u001b[39m     request: Any,\n\u001b[32m   (...)\u001b[39m\u001b[32m    275\u001b[39m     compression: Optional[grpc.Compression] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    276\u001b[39m ) -> Any:\n\u001b[32m--> \u001b[39m\u001b[32m277\u001b[39m     response, ignored_call = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_with_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    278\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    279\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    280\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    281\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcredentials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    282\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwait_for_ready\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwait_for_ready\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    283\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    284\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    285\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\prana\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\grpc\\_interceptor.py:329\u001b[39m, in \u001b[36m_UnaryUnaryMultiCallable._with_call\u001b[39m\u001b[34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[39m\n\u001b[32m    326\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exception:  \u001b[38;5;66;03m# pylint:disable=broad-except\u001b[39;00m\n\u001b[32m    327\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m _FailureOutcome(exception, sys.exc_info()[\u001b[32m2\u001b[39m])\n\u001b[32m--> \u001b[39m\u001b[32m329\u001b[39m call = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_interceptor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mintercept_unary_unary\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    330\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontinuation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclient_call_details\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\n\u001b[32m    331\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    332\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m call.result(), call\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\prana\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\google\\ai\\generativelanguage_v1beta\\services\\generative_service\\transports\\grpc.py:79\u001b[39m, in \u001b[36m_LoggingClientInterceptor.intercept_unary_unary\u001b[39m\u001b[34m(self, continuation, client_call_details, request)\u001b[39m\n\u001b[32m     64\u001b[39m     grpc_request = {\n\u001b[32m     65\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mpayload\u001b[39m\u001b[33m\"\u001b[39m: request_payload,\n\u001b[32m     66\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mrequestMethod\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mgrpc\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     67\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mdict\u001b[39m(request_metadata),\n\u001b[32m     68\u001b[39m     }\n\u001b[32m     69\u001b[39m     _LOGGER.debug(\n\u001b[32m     70\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSending request for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclient_call_details.method\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     71\u001b[39m         extra={\n\u001b[32m   (...)\u001b[39m\u001b[32m     76\u001b[39m         },\n\u001b[32m     77\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m response = \u001b[43mcontinuation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclient_call_details\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m logging_enabled:  \u001b[38;5;66;03m# pragma: NO COVER\u001b[39;00m\n\u001b[32m     81\u001b[39m     response_metadata = response.trailing_metadata()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\prana\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\grpc\\_interceptor.py:315\u001b[39m, in \u001b[36m_UnaryUnaryMultiCallable._with_call.<locals>.continuation\u001b[39m\u001b[34m(new_details, request)\u001b[39m\n\u001b[32m    306\u001b[39m (\n\u001b[32m    307\u001b[39m     new_method,\n\u001b[32m    308\u001b[39m     new_timeout,\n\u001b[32m   (...)\u001b[39m\u001b[32m    312\u001b[39m     new_compression,\n\u001b[32m    313\u001b[39m ) = _unwrap_client_call_details(new_details, client_call_details)\n\u001b[32m    314\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m315\u001b[39m     response, call = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_thunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_method\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwith_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    316\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    317\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    318\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    319\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_credentials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    320\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwait_for_ready\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_wait_for_ready\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    321\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_compression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    322\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    323\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _UnaryOutcome(response, call)\n\u001b[32m    324\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m grpc.RpcError \u001b[38;5;28;01mas\u001b[39;00m rpc_error:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\prana\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\grpc\\_channel.py:1192\u001b[39m, in \u001b[36m_UnaryUnaryMultiCallable.with_call\u001b[39m\u001b[34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[39m\n\u001b[32m   1183\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwith_call\u001b[39m(\n\u001b[32m   1184\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1185\u001b[39m     request: Any,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1190\u001b[39m     compression: Optional[grpc.Compression] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1191\u001b[39m ) -> Tuple[Any, grpc.Call]:\n\u001b[32m-> \u001b[39m\u001b[32m1192\u001b[39m     state, call = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_blocking\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1193\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait_for_ready\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompression\u001b[49m\n\u001b[32m   1194\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1195\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _end_unary_response_blocking(state, call, \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\prana\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\grpc\\_channel.py:1165\u001b[39m, in \u001b[36m_UnaryUnaryMultiCallable._blocking\u001b[39m\u001b[34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[39m\n\u001b[32m   1148\u001b[39m state.target = _common.decode(\u001b[38;5;28mself\u001b[39m._target)\n\u001b[32m   1149\u001b[39m call = \u001b[38;5;28mself\u001b[39m._channel.segregated_call(\n\u001b[32m   1150\u001b[39m     cygrpc.PropagationConstants.GRPC_PROPAGATE_DEFAULTS,\n\u001b[32m   1151\u001b[39m     \u001b[38;5;28mself\u001b[39m._method,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1163\u001b[39m     \u001b[38;5;28mself\u001b[39m._registered_call_handle,\n\u001b[32m   1164\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1165\u001b[39m event = \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnext_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1166\u001b[39m _handle_event(event, state, \u001b[38;5;28mself\u001b[39m._response_deserializer)\n\u001b[32m   1167\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m state, call\n",
      "\u001b[36mFile \u001b[39m\u001b[32msrc/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi:388\u001b[39m, in \u001b[36mgrpc._cython.cygrpc.SegregatedCall.next_event\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32msrc/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi:211\u001b[39m, in \u001b[36mgrpc._cython.cygrpc._next_call_event\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32msrc/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi:205\u001b[39m, in \u001b[36mgrpc._cython.cygrpc._next_call_event\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32msrc/python/grpcio/grpc/_cython/_cygrpc/completion_queue.pyx.pxi:97\u001b[39m, in \u001b[36mgrpc._cython.cygrpc._latent_event\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32msrc/python/grpcio/grpc/_cython/_cygrpc/completion_queue.pyx.pxi:80\u001b[39m, in \u001b[36mgrpc._cython.cygrpc._internal_latent_event\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32msrc/python/grpcio/grpc/_cython/_cygrpc/completion_queue.pyx.pxi:61\u001b[39m, in \u001b[36mgrpc._cython.cygrpc._next\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# =============================\n",
    "# Generate 10 Diverse Datasets\n",
    "# =============================\n",
    "\n",
    "# Define 10 different configurations for maximum diversity\n",
    "DATASET_CONFIGS = [\n",
    "    {\"student\": \"excellent\", \"difficulty\": \"hard\", \"exchanges\": 8},      # 1: Expert on complex problems (longest)\n",
    "    {\"student\": \"struggling\", \"difficulty\": \"easy\", \"exchanges\": 4},     # 2: Beginner on simple topics (shortest)\n",
    "    {\"student\": \"average\", \"difficulty\": \"medium\", \"exchanges\": 6},      # 3: Typical mid-level interview\n",
    "    {\"student\": \"excellent\", \"difficulty\": \"easy\", \"exchanges\": 6},      # 4: Expert breezing through basics\n",
    "    {\"student\": \"struggling\", \"difficulty\": \"hard\", \"exchanges\": 6},     # 5: Weak candidate struggling with complexity\n",
    "    {\"student\": \"average\", \"difficulty\": \"easy\", \"exchanges\": 8},        # 6: Average student on simple topics (detailed)\n",
    "    {\"student\": \"excellent\", \"difficulty\": \"medium\", \"exchanges\": 4},    # 7: Expert giving quick scalable answers\n",
    "    {\"student\": \"struggling\", \"difficulty\": \"medium\", \"exchanges\": 8},   # 8: Struggling candidate on moderate problems\n",
    "    {\"student\": \"average\", \"difficulty\": \"hard\", \"exchanges\": 4},        # 9: Average student facing tough questions\n",
    "    {\"student\": \"excellent\", \"difficulty\": \"hard\", \"exchanges\": 6},      # 10: Expert on complex systems (balanced)\n",
    "]\n",
    "\n",
    "def generate_single_dataset(student_level: str, difficulty: str, num_exchanges: int) -> str:\n",
    "    \"\"\"Generate a single interview transcript with given configuration.\"\"\"\n",
    "    \n",
    "    # Create models for this specific configuration\n",
    "    interviewer_model = make_model(INTERVIEWER_DIFFICULTY_PROMPTS[difficulty])\n",
    "    candidate_model = make_model(STUDENT_PROMPTS[student_level])\n",
    "    \n",
    "    # Select random kickoff question\n",
    "    kickoff = random.choice(KICKOFF_QUESTION_BANK[difficulty])\n",
    "    \n",
    "    # Initialize conversation history\n",
    "    history = [kickoff]\n",
    "    \n",
    "    # Generate conversation\n",
    "    for _ in range(num_exchanges - 1):\n",
    "        # Candidate answers\n",
    "        last_question = history[-1]\n",
    "        cand_raw = call_with_retry(candidate_model, last_question)\n",
    "        \n",
    "        # Apply hesitation with current student level context\n",
    "        cand_raw = cand_raw.strip().replace('\\n', ' ')\n",
    "        cand_raw = re.sub(r'^candidate:\\s*', '', cand_raw, flags=re.I)\n",
    "        cand_raw = trim_response_length(cand_raw, max_sentences=1, max_words=18)\n",
    "        \n",
    "        # Apply hesitation patterns\n",
    "        cfg = HESITATION_PATTERNS[student_level]\n",
    "        lower = cand_raw.lower()\n",
    "        starts_hes = any(lower.startswith(p.split('...')[0].strip(' .').lower()) for p in cfg[\"patterns\"])\n",
    "        if starts_hes and random.random() > cfg[\"rate\"]:\n",
    "            cand_raw = re.sub(r'^(right|actually|hmm+|um+|uh+|i think|maybe|probably|i guess|let me think)[, .\\-–—]*', '', cand_raw, flags=re.I).lstrip()\n",
    "        elif not starts_hes and random.random() < cfg[\"rate\"]:\n",
    "            cand_raw = f\"{random.choice(cfg['patterns'])} {cand_raw}\".strip()\n",
    "        \n",
    "        cand_msg = ensure_prefix(cand_raw, \"Candidate:\")\n",
    "        history.append(cand_msg)\n",
    "        \n",
    "        # Interviewer asks follow-up\n",
    "        context = \"\\n\".join(history)\n",
    "        int_raw = call_with_retry(interviewer_model, context)\n",
    "        int_raw = int_raw.strip().replace('\\n', ' ')\n",
    "        int_raw = re.sub(r'^interviewer:\\s*', '', int_raw, flags=re.I)\n",
    "        int_raw = trim_response_length(int_raw, max_sentences=1, max_words=15)\n",
    "        int_msg = ensure_prefix(int_raw, \"Interviewer:\")\n",
    "        history.append(int_msg)\n",
    "    \n",
    "    # Format the transcript\n",
    "    transcript = \"\\n\".join(history)\n",
    "    metadata = f\"\\n--- Difficulty: {difficulty.upper()} | Student Level: {student_level.upper()} | Exchanges: {num_exchanges} ---\"\n",
    "    \n",
    "    return transcript + metadata\n",
    "\n",
    "# Generate all 10 datasets\n",
    "print(\"Starting dataset generation...\\n\")\n",
    "all_datasets = []\n",
    "\n",
    "for idx, config in enumerate(tqdm(DATASET_CONFIGS, desc=\"Generating datasets\"), start=1):\n",
    "    print(f\"\\nGenerating Dataset {idx}: {config['student']} + {config['difficulty']} + {config['exchanges']} exchanges\")\n",
    "    \n",
    "    dataset = generate_single_dataset(\n",
    "        student_level=config[\"student\"],\n",
    "        difficulty=config[\"difficulty\"],\n",
    "        num_exchanges=config[\"exchanges\"]\n",
    "    )\n",
    "    \n",
    "    all_datasets.append(f\"========== DATASET {idx} ==========\\n{dataset}\\n\")\n",
    "    print(f\"✓ Dataset {idx} generated successfully!\")\n",
    "\n",
    "# Save all datasets to a single file\n",
    "output_file = \"all_datasets.txt\"\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    f.write(\"\\n\\n\".join(all_datasets))\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"✅ SUCCESS! All 10 datasets generated and saved to: {output_file}\")\n",
    "print(f\"{'='*60}\")\n",
    "print(\"\\nDataset Summary:\")\n",
    "for idx, config in enumerate(DATASET_CONFIGS, start=1):\n",
    "    print(f\"  {idx}. {config['student'].capitalize():12} | {config['difficulty'].capitalize():6} | {config['exchanges']} exchanges\")\n",
    "print(f\"\\nTotal file size: {os.path.getsize(output_file) / 1024:.2f} KB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93ef467f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting JSON dataset generation for 5 samples (each with exchanges between 6 and 10)...\n",
      "\n",
      "📂 Found existing file with 45 samples. Will append new ones.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating 5 JSON datasets:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\prana\\AppData\\Local\\Temp\\ipykernel_2932\\3978103227.py:80: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  \"generated_at\": datetime.utcnow().isoformat() + \"Z\",\n",
      "Generating 5 JSON datasets:  20%|██        | 1/5 [01:49<07:16, 109.02s/it]C:\\Users\\prana\\AppData\\Local\\Temp\\ipykernel_2932\\3978103227.py:80: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  \"generated_at\": datetime.utcnow().isoformat() + \"Z\",\n",
      "Generating 5 JSON datasets: 100%|██████████| 5/5 [08:30<00:00, 102.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "✅ SUCCESS! Generated 5 new datasets\n",
      "📊 Total datasets in file: 50\n",
      "💾 Saved to: all_datasets.json\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# =============================\n",
    "# Generate 10 JSON Datasets (exchanges >= 6)\n",
    "# =============================\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Configurable range for exchanges (must be >= 6 as requested)\n",
    "MIN_EXCHANGES, MAX_EXCHANGES = 6, 10\n",
    "SAMPLE_COUNT = 5 # Generate 10 samples per run\n",
    "OUTPUT_JSON = \"all_datasets.json\"\n",
    "\n",
    "LEVEL_CHOICES = [\"excellent\", \"average\", \"struggling\"]\n",
    "DIFFICULTY_CHOICES = [\"easy\", \"medium\", \"hard\"]\n",
    "\n",
    "\n",
    "def to_turn(s: str) -> dict:\n",
    "    \"\"\"Convert a prefixed line (e.g., 'Interviewer: ...') into a structured turn.\"\"\"\n",
    "    s = s.strip()\n",
    "    low = s.lower()\n",
    "    if low.startswith(\"interviewer:\"):\n",
    "        return {\"speaker\": \"Interviewer\", \"text\": s.split(\":\", 1)[1].strip()}\n",
    "    if low.startswith(\"candidate:\"):\n",
    "        return {\"speaker\": \"Candidate\", \"text\": s.split(\":\", 1)[1].strip()}\n",
    "    return {\"speaker\": \"Unknown\", \"text\": s}\n",
    "\n",
    "\n",
    "def generate_dataset_dict(student_level: str, difficulty: str, num_exchanges: int, sample_id: int) -> dict:\n",
    "    \"\"\"Generate one dataset as a structured dict, without modifying global STUDENT_LEVEL.\"\"\"\n",
    "    interviewer_model = make_model(INTERVIEWER_DIFFICULTY_PROMPTS[difficulty])\n",
    "    candidate_model = make_model(STUDENT_PROMPTS[student_level])\n",
    "\n",
    "    # Kickoff\n",
    "    kickoff_full = random.choice(KICKOFF_QUESTION_BANK[difficulty])\n",
    "    kickoff_text = kickoff_full.split(\":\", 1)[1].strip() if \":\" in kickoff_full else kickoff_full\n",
    "\n",
    "    history: List[str] = [kickoff_full]\n",
    "\n",
    "    # Conversation generation (num_exchanges includes the kickoff interviewer turn)\n",
    "    for _ in range(num_exchanges - 1):\n",
    "        # Candidate answers\n",
    "        last_question = history[-1]\n",
    "        cand_raw = call_with_retry(candidate_model, last_question)\n",
    "        cand_raw = cand_raw.strip().replace(\"\\n\", \" \")\n",
    "        cand_raw = re.sub(r'^candidate:\\s*', '', cand_raw, flags=re.I)\n",
    "        cand_raw = trim_response_length(cand_raw, max_sentences=1, max_words=18)\n",
    "\n",
    "        # Apply hesitation based on this dataset's student level (not global STUDENT_LEVEL)\n",
    "        cfg = HESITATION_PATTERNS[student_level]\n",
    "        lower = cand_raw.lower()\n",
    "        starts_hes = any(lower.startswith(p.split('...')[0].strip(' .').lower()) for p in cfg[\"patterns\"])\n",
    "        if starts_hes and random.random() > cfg[\"rate\"]:\n",
    "            cand_raw = re.sub(r'^(right|actually|hmm+|um+|uh+|i think|maybe|probably|i guess|let me think)[, .\\-–—]*', '', cand_raw, flags=re.I).lstrip()\n",
    "        elif not starts_hes and random.random() < cfg[\"rate\"]:\n",
    "            cand_raw = f\"{random.choice(cfg['patterns'])} {cand_raw}\".strip()\n",
    "\n",
    "        cand_msg = ensure_prefix(cand_raw, \"Candidate:\")\n",
    "        history.append(cand_msg)\n",
    "\n",
    "        # Interviewer follow-up\n",
    "        context = \"\\n\".join(history)\n",
    "        int_raw = call_with_retry(interviewer_model, context)\n",
    "        int_raw = int_raw.strip().replace('\\n', ' ')\n",
    "        int_raw = re.sub(r'^interviewer:\\s*', '', int_raw, flags=re.I)\n",
    "        int_raw = trim_response_length(int_raw, max_sentences=1, max_words=15)\n",
    "        int_msg = ensure_prefix(int_raw, \"Interviewer:\")\n",
    "        history.append(int_msg)\n",
    "\n",
    "    # Build structured turns\n",
    "    turns = [to_turn(line) for line in history]\n",
    "\n",
    "    return {\n",
    "        \"id\": sample_id,\n",
    "        \"student_level\": student_level,\n",
    "        \"difficulty\": difficulty,\n",
    "        \"exchanges\": num_exchanges,\n",
    "        \"kickoff_question\": kickoff_text,\n",
    "        \"turns\": turns,\n",
    "        \"meta\": {\n",
    "            \"model\": MODEL_NAME,\n",
    "            \"generated_at\": datetime.utcnow().isoformat() + \"Z\",\n",
    "        },\n",
    "    }\n",
    "\n",
    "\n",
    "print(f\"Starting JSON dataset generation for {SAMPLE_COUNT} samples (each with exchanges between {MIN_EXCHANGES} and {MAX_EXCHANGES})...\\n\")\n",
    "\n",
    "# Check if file exists and load existing data\n",
    "existing_datasets = []\n",
    "if os.path.exists(OUTPUT_JSON):\n",
    "    try:\n",
    "        with open(OUTPUT_JSON, \"r\", encoding=\"utf-8\") as f:\n",
    "            existing_datasets = json.load(f)\n",
    "        print(f\"📂 Found existing file with {len(existing_datasets)} samples. Will append new ones.\\n\")\n",
    "    except:\n",
    "        print(f\"⚠️  Existing file found but couldn't read it. Starting fresh.\\n\")\n",
    "\n",
    "# Determine starting ID\n",
    "start_id = len(existing_datasets) + 1\n",
    "\n",
    "datasets = []\n",
    "for i in tqdm(range(start_id, start_id + SAMPLE_COUNT), desc=f\"Generating {SAMPLE_COUNT} JSON datasets\"):\n",
    "    level = random.choice(LEVEL_CHOICES)\n",
    "    diff = random.choice(DIFFICULTY_CHOICES)\n",
    "    exchanges = random.randint(MIN_EXCHANGES, MAX_EXCHANGES)\n",
    "\n",
    "    ds = generate_dataset_dict(level, diff, exchanges, i)\n",
    "    datasets.append(ds)\n",
    "\n",
    "# Combine with existing data\n",
    "all_datasets = existing_datasets + datasets\n",
    "\n",
    "# Save as a single JSON array file\n",
    "with open(OUTPUT_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(all_datasets, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"✅ SUCCESS! Generated {SAMPLE_COUNT} new datasets\")\n",
    "print(f\"📊 Total datasets in file: {len(all_datasets)}\")\n",
    "print(f\"💾 Saved to: {OUTPUT_JSON}\")\n",
    "print(f\"{'='*60}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
